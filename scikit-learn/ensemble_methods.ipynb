{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "The list of all the functions along with their documentations is given below:\n",
    "\n",
    "- [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "- [BaggingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)\n",
    "- [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "- [RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "- [ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)\n",
    "- [ExtraTreesRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)\n",
    "- [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "- [AdaBoostRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble, neighbors, datasets, linear_model, naive_bayes, svm, neighbors, tree\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "Short for bootstrap aggregation. In scikit-learn, bagging methods are offered as a unified `BaggingClassifier` meta-estimator (resp. `BaggingRegressor`), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, `max_samples` and `max_features` control the size of the subsets (in terms of samples and features), while `bootstrap` and `bootstrap_features` control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting `oob_score=True`. **Bagging can also be parallelized using teh `n_jobs` argument.**\n",
    "\n",
    "#### Fitting a bagging model\n",
    "Fitting a KNN model on iris data set using bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging = ensemble.BaggingClassifier(neighbors.KNeighborsClassifier(), max_samples=0.5, \n",
    "                            max_features=0.5, oob_score = True, \n",
    "                            random_state = 2017-12-29)\n",
    "\n",
    "n_neighbors = 15\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "[0 1 2]\n",
      "0.946666666667\n"
     ]
    }
   ],
   "source": [
    "bagging.fit(X,y)\n",
    "print(bagging.base_estimator_) # the base estimator\n",
    "print(bagging.classes_) # class labels\n",
    "print(bagging.oob_score_) # out of bag success rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction using bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 49  1]\n",
      " [ 0  1 49]]\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "pred = bagging.predict(X)\n",
    "cmat = confusion_matrix(y, pred)\n",
    "accuracy = round(100*cmat.diagonal().sum()/cmat.sum())\n",
    "print(cmat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using bagging\n",
    "Regression can also be done in a similar way by just using the base estimator as a regression model in `ensemble.BaggingRegressor`. One example is provided [here](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py).\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "`RandomForestClassifier` and `RandomForestRegressor` are used for classification and regression respectively. The importance of features is stored in the attribute `feature_importances_` of the fitted object of the above two classes.\n",
    "\n",
    "Each tree in the ensemble is built from a bootstrap sample drawn from the training set. The split is picked as the best split among a random subset of the features, instead of all features.\n",
    "\n",
    "## Extra Trees\n",
    "\n",
    "Short for Extremely Randomized Trees. `ExtraTreesClassifier` and `ExtraTreesRegressor` are used for classification and regression respectively.\n",
    "\n",
    "In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.\n",
    "\n",
    "## Best Choices for Parameters\n",
    "\n",
    "The main parameters to adjust when using these methods is `n_estimators` and `max_features`. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. **Empirical good default values are `max_features=n_features` for regression problems, and `max_features=sqrt(n_features)` for classification** tasks (where `n_features` is the number of features in the data). Good results are often achieved when setting **`max_depth=None`** in combination with **`min_samples_split=2`** (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. **The best parameter values should always be cross-validated**. In addition, note that **in random forests, bootstrap samples are used by default (`bootstrap=True`) while the default strategy for extra-trees is to use the whole dataset (`bootstrap=False`)**. When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting `oob_score=True`.\n",
    "\n",
    "## Parallelization\n",
    "\n",
    "Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the n_jobs parameter. If n_jobs=k then computations are partitioned into k jobs, and run on k cores of the machine. If n_jobs=-1 then all cores available on the machine are used.\n",
    "\n",
    "## Feature importance evaluation\n",
    "\n",
    "[Example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py)\n",
    "\n",
    "The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. In practice those estimates are stored as an attribute named `feature_importances_` on the fitted model. This is an array with shape (`n_features`,) whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.430877319766\n",
      "MSPE for random forest regressor = 2871.62210986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.06379473,  0.01113133,  0.27151075,  0.09616855,  0.04525399,\n",
       "        0.05564851,  0.05086904,  0.02331249,  0.30735058,  0.07496003])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X_train = diabetes.data[:-20]\n",
    "X_test = diabetes.data[-20:]\n",
    "y_train = diabetes.target[:-20]\n",
    "y_test = diabetes.target[-20:]\n",
    "rf = ensemble.RandomForestRegressor(n_estimators = 1000, oob_score = True, n_jobs = -1, random_state = 0)\n",
    "rf.fit(X_train,y_train)\n",
    "print(rf.oob_score_)\n",
    "pred = rf.predict(X_test)\n",
    "print(\"MSPE for random forest regressor =\", mean_squared_error(y_test, pred))\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21439479240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances) # sorts the indices of the array, like 'order' in R\n",
    "importances = importances[indices] # putting importances in the sorted order\n",
    "\n",
    "# plotting the barplot horizontally\n",
    "plt.barh(range(len(importances)),importances, color = \"red\")\n",
    "plt.title(\"Importance of Features\")\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.yticks(range(len(importances)), indices)\n",
    "for i, v in enumerate(importances): # displaying the relative importance alongside each bar in the plot\n",
    "    plt.text(v + 0.001, i - 0.1, str(int(round(v,2)*100)), fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSPE for random forest regressor = 2327.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:723: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    }
   ],
   "source": [
    "extra = ensemble.ExtraTreesRegressor(n_estimators = 10, bootstrap = True, oob_score = True, n_jobs = -1, random_state = 0)\n",
    "extra.fit(X_train,y_train)\n",
    "extra.oob_score_\n",
    "pred = extra.predict(X_test)\n",
    "print(\"MSPE for random forest regressor =\", mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might sometimes get a warning in computing `oob_score_` because it is possible that while resampling using bootstrap, some observations get selected in all the samples. These observations will never be \"out-of-bag\" and hence, we cannot calculate the OOB estimate for these observations.\n",
    "\n",
    "Extra trees perform much better than random forest but still not better than linear regression. This model could be improved by parameter tuning, like cross-validation or grid search.\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights $w_1$, $w_2$, $\\dots$, $w_N$ to each of the training samples. Initially, those weights are all set to $w_i = 1/N$, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.\n",
    "\n",
    "[`AdaBoostClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for classification, [`AdaBoostRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95996732026143794"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "clf = ensemble.AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, iris.data, iris.target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters in AdaBoost\n",
    "\n",
    "The number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the base_estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples at a leaf min_samples_leaf in case of decision trees).\n",
    "\n",
    "## Gradient Tree Boosting\n",
    "\n",
    "[GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) for classification and [GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) for regression.\n",
    "\n",
    "### Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91300000000000003"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = datasets.make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For datasets with a large number of classes we strongly recommend to use RandomForestClassifier as an alternative to GradientBoostingClassifier.**\n",
    "\n",
    "### Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0091548599603213"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = datasets.make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = ensemble.GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, \n",
    "                                         random_state=0, loss='ls')\n",
    "est.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier\n",
    "\n",
    "The idea behind the `VotingClassifier` is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. \n",
    "\n",
    "### Hard Voting\n",
    "\n",
    "The `VotingClassifier` (with `voting='hard'`) would classify the sample as “class 1” based on the majority class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.93 (+/- 0.05) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.05) [Voting Classifier]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = linear_model.LogisticRegression(random_state=1)\n",
    "clf2 = ensemble.RandomForestClassifier(random_state=1)\n",
    "clf3 = naive_bayes.GaussianNB()\n",
    "\n",
    "eclf = ensemble.VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Voting Classifier']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting\n",
    "In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0,2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = svm.SVC(kernel='rbf', probability=True)\n",
    "eclf = ensemble.VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2,1,2])\n",
    "\n",
    "clf1 = clf1.fit(X,y)\n",
    "clf2 = clf2.fit(X,y)\n",
    "clf3 = clf3.fit(X,y)\n",
    "eclf = eclf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the VotingClassifier with GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = linear_model.LogisticRegression(random_state=1)\n",
    "clf2 = ensemble.RandomForestClassifier(random_state=1)\n",
    "clf3 = naive_bayes.GaussianNB()\n",
    "eclf = ensemble.VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
