{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques\n",
    "Some common data preprocessing techniques are outlined here. The `preprocessing` library in `sklearn` is the main library used for preprocessing. The functions used here are listed below with their documentations:\n",
    "\n",
    "- [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[ 0.  0.  0.]\n",
      "[ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "print(X_scaled)  \n",
    "print(X_scaled.mean(axis = 0)) # axis = 0 means evaluated along a column\n",
    "print(X_scaled.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `StandardScaler` function\n",
    "When we want to scale the test set, we must use the mean and variance computed from the training set. We cannot use the mean and variance from the test set because those would be different for different test sets and hence introduce bias. The `StandardScaler` function can be used to scale different data sets using the mean and variance computed from the training set. To disable either centering or scaling, set the `with_mean` or `with_std` parameters to `False` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[ 1.          0.          0.33333333]\n",
      "[ 0.81649658  0.81649658  1.24721913]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first fit the Standard Scaler to compute the mean and variance\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "print(scaler)\n",
    "print(scaler.mean_) \n",
    "print(scaler.scale_)   \n",
    "\n",
    "# Then we can use transform to scale both the training and test sets\n",
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler instance can then be used on new data to transform it the same way it did on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.44948974,  1.22474487, -0.26726124]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [[-1., 1., 0.]]\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other scaler functions include [`MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) and [`MaxAbsScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler).\n",
    "\n",
    "`MaxAbsScaler` is recommended for scaling sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization\n",
    "Everything below a threshold becomes 0 and everything above it becomes 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "      [ 2.,  0.,  0.],\n",
    "      [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer(threshold=1.1).fit(X)  # default threshold is 0\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "[`OneHotEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "To convert categorical features into features that can be used with scikit-learn estimators, we can use a one-of-$K$ or one-hot encoding, which is implemented in `OneHotEncoder` function. This estimator transforms each categorical feature with $m$ possible values into $m$ binary features, with only one active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \n",
    "enc.transform([[0, 1, 3]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original data, the first column has two categories so it is represented by the first two columns in the one hot encoded data. Similarly, the second and the third variables have 3 and 4 possible classes and hence are represented by 3 and 4 columns in the one hot encoded data with the corresponding columns having 1 and the rest 0s.\n",
    "\n",
    "Note that, if there is a possibility that the training data might have missing categorical features, one has to explicitly set `n_values`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(n_values=[2, 3, 4])\n",
    "# Note that there are missing categorical values for the 2nd and 3rd features\n",
    "enc.fit([[1, 2, 3], [0, 2, 0]])\n",
    "enc.transform([[1, 0, 0]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "[`Imputer`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer)\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean value of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          2.        ]\n",
      " [ 6.          3.66666667]\n",
      " [ 7.          6.        ]]\n"
     ]
    }
   ],
   "source": [
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better imputation [example](http://scikit-learn.org/stable/auto_examples/plot_missing_values.html#sphx-glr-auto-examples-plot-missing-values-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating polynomial features\n",
    "[preprocessing.PolynomialFeatures()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   3.,   4.,   6.,   9.],\n",
       "       [  1.,   4.,   5.,  16.,  20.,  25.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)\n",
    "poly = preprocessing.PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is a bias vector (the constant vector with all ones).\n",
    "\n",
    "To find out the names of the features generated, to indicate which column is a power and which one is an interaction term, we can proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers\n",
    "Implement a transformer from an arbitrary function with [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.]\n",
      "[[ 0.          1.        ]\n",
      " [ 1.41421356  1.73205081]]\n"
     ]
    }
   ],
   "source": [
    "# taking column wise mean (just to show how the function works) \n",
    "transformer = preprocessing.FunctionTransformer(np.mean, kw_args={'axis':0}) \n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "print(transformer.transform(X))\n",
    "\n",
    "# taking square root (more practical transformation)\n",
    "transformer = preprocessing.FunctionTransformer(np.sqrt) \n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "print(transformer.transform(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
