{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Documentation: [linear_model.LogisticRegression()](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing hand written digits\n",
    "The digits data set description is [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits). There are 64 features for each data point to represent the 64 pixels in an 8 x 8 image. We will build a logistic regression model to identify whether the digit is greater than 4 or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s `transform`'s `fit()` just calculates the parameters (e.g. $\\mu$ and $\\sigma$ in case of `StandardScaler`) and saves them as an internal objects state. Afterwards, you can call its `transform()` method to apply the transformation to a particular set of examples.\n",
    "\n",
    "`fit_transform()` joins these two steps and is used for the initial fitting of parameters on the training set `X`, but it also returns a transformed `X`. Internally, it just calls `fit()` first and then `transform()` on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = digits.data, digits.target\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify small against large digits\n",
    "y = (y > 4).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`C` is the inverse of regularization strength and it must be a positive float. Smaller values specify stronger regularization. Logistic regression always needs a penalty to work, either $L^1$ or $L^2$ penalty. $L^1$ is similar to Lasso and $L^2$ is similar to Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 4.69%\n",
      "score with L1 penalty: 0.9098\n",
      "Sparsity with L2 penalty: 4.69%\n",
      "score with L2 penalty: 0.9098\n",
      "\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 9.38%\n",
      "score with L1 penalty: 0.9104\n",
      "Sparsity with L2 penalty: 4.69%\n",
      "score with L2 penalty: 0.9093\n",
      "\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 85.94%\n",
      "score with L1 penalty: 0.8625\n",
      "Sparsity with L2 penalty: 4.69%\n",
      "score with L2 penalty: 0.8915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set regularization parameter\n",
    "for i, C in enumerate((100, 1, 0.01)):  # i = 0, 1, 2 and C = 100, 1, 0.01\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X, y)\n",
    "    clf_l2_LR.fit(X, y)\n",
    "    \n",
    "    # ravel creates flattened numpy arrays\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "    \n",
    "    # percentage of coefficients which are zero\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "    \n",
    "    # score is the R^2 value\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, y))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\\n\" % clf_l2_LR.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
