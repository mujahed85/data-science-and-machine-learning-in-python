{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The functions used here are listed below:\n",
    "\n",
    "- [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)\n",
    "- [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)\n",
    "- [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm, preprocessing, datasets\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn a random split into training and test sets can be quickly computed with the `train_test_split` helper function. `train_size` and `test_size` parameters can be used to specify the sizes of training and test sets. The default is 75% in train and 25% in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n",
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape, iris.target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, \n",
    "                                                                    test_size=0.4, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation using `cross_val_score`\n",
    "\n",
    "The simplest way to use cross-validation is to call the `cross_val_score` helper function on the estimator and the dataset. **It can be parallelized using `n_jobs` argument.It preserves the percentage of samples for each class for labeled data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n",
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the `scoring` parameter.\n",
    "\n",
    "### Cross Validation using `cross_validate`\n",
    "\n",
    "The `cross_validate` function differs from cross_val_score in two ways -\n",
    "\n",
    "It allows specifying multiple metrics for evaluation.\n",
    "It returns a dict containing training scores, fit-times and score-times in addition to the test score.\n",
    "\n",
    "Here is [an example](http://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation) showing the usage of `corss_validate`.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "A pipeline of transforms with a final estimator. Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. \n",
    "\n",
    "### Without Pipeline\n",
    "\n",
    "Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied to held-out data for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "clf.score(X_test_transformed, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n",
      "[ 1.          0.93333333  1.          1.          1.          0.93333333\n",
      "  0.93333333  1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "pipe.fit(X_train, y_train)\n",
    "print(pipe.score(X_test, y_test))\n",
    "print(cross_val_score(clf, iris.data, iris.target, cv=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper parameters of an estimator\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, `GridSearchCV` exhaustively considers all parameter combinations, while `RandomizedSearchCV` can sample a given number of candidates from a parameter space with a specified distribution.\n",
    "\n",
    "### Exhaustive Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.07720406,  0.02886918,  0.06144266,  0.02776921,  0.0733516 ,\n",
       "         0.02796974,  0.03527462,  0.03427384,  0.06794791,  0.03367379,\n",
       "         0.03572505,  0.04703333,  0.12724025,  0.06049283,  0.07220111,\n",
       "         0.05754068]),\n",
       " 'mean_score_time': array([ 0.00610518,  0.00405285,  0.00740519,  0.00370264,  0.00755544,\n",
       "         0.00290213,  0.00410295,  0.00290256,  0.00580423,  0.00420294,\n",
       "         0.00390267,  0.00740511,  0.01075768,  0.00535431,  0.01145835,\n",
       "         0.00550413]),\n",
       " 'mean_test_score': array([ 0.98329621,  0.97438753,  0.95879733,  0.97438753,  0.98329621,\n",
       "         0.97438753,  0.98106904,  0.97438753,  0.98329621,  0.97438753,\n",
       "         0.98218263,  0.97438753,  0.98329621,  0.97438753,  0.98218263,\n",
       "         0.97438753]),\n",
       " 'mean_train_score': array([ 0.99888658,  1.        ,  0.96881809,  1.        ,  1.        ,\n",
       "         1.        ,  0.99814385,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'param_C': masked_array(data = [1 1 1 1 10 10 10 10 100 100 100 100 1000 1000 1000 1000],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_gamma': masked_array(data = [0.001 0.001 0.0001 0.0001 0.001 0.001 0.0001 0.0001 0.001 0.001 0.0001\n",
       "  0.0001 0.001 0.001 0.0001 0.0001],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_kernel': masked_array(data = ['rbf' 'linear' 'rbf' 'linear' 'rbf' 'linear' 'rbf' 'linear' 'rbf' 'linear'\n",
       "  'rbf' 'linear' 'rbf' 'linear' 'rbf' 'linear'],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.001, 'kernel': 'linear'},\n",
       "  {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'linear'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'},\n",
       "  {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.001, 'kernel': 'linear'},\n",
       "  {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'},\n",
       "  {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'},\n",
       "  {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.0001, 'kernel': 'linear'}),\n",
       " 'rank_test_score': array([ 1,  8, 16,  8,  1,  8,  7,  8,  1,  8,  5,  8,  1,  8,  5,  8]),\n",
       " 'split0_test_score': array([ 0.98947368,  0.96842105,  0.98947368,  0.96842105,  0.98947368,\n",
       "         0.96842105,  0.98947368,  0.96842105,  0.98947368,  0.96842105,\n",
       "         1.        ,  0.96842105,  0.98947368,  0.96842105,  1.        ,\n",
       "         0.96842105]),\n",
       " 'split0_train_score': array([ 0.99875467,  1.        ,  0.96886675,  1.        ,  1.        ,\n",
       "         1.        ,  0.99875467,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.9893617 ,  0.9787234 ,  0.96808511,  0.9787234 ,  0.9893617 ,\n",
       "         0.9787234 ,  1.        ,  0.9787234 ,  0.9893617 ,  0.9787234 ,\n",
       "         1.        ,  0.9787234 ,  0.9893617 ,  0.9787234 ,  1.        ,\n",
       "         0.9787234 ]),\n",
       " 'split1_train_score': array([ 0.99875622,  1.        ,  0.96890547,  1.        ,  1.        ,\n",
       "         1.        ,  0.99875622,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.97802198,  0.97802198,  0.96703297,  0.97802198,  0.97802198,\n",
       "         0.97802198,  0.97802198,  0.97802198,  0.97802198,  0.97802198,\n",
       "         0.97802198,  0.97802198,  0.97802198,  0.97802198,  0.97802198,\n",
       "         0.97802198]),\n",
       " 'split2_train_score': array([ 1.        ,  1.        ,  0.96902107,  1.        ,  1.        ,\n",
       "         1.        ,  0.99876084,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split3_test_score': array([ 1.        ,  0.98901099,  0.96703297,  0.98901099,  1.        ,\n",
       "         0.98901099,  0.97802198,  0.98901099,  1.        ,  0.98901099,\n",
       "         0.98901099,  0.98901099,  1.        ,  0.98901099,  0.98901099,\n",
       "         0.98901099]),\n",
       " 'split3_train_score': array([ 0.99876084,  1.        ,  0.96530359,  1.        ,  1.        ,\n",
       "         1.        ,  0.99504337,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split4_test_score': array([ 0.97777778,  0.97777778,  0.97777778,  0.97777778,  0.97777778,\n",
       "         0.97777778,  0.97777778,  0.97777778,  0.97777778,  0.97777778,\n",
       "         0.97777778,  0.97777778,  0.97777778,  0.97777778,  0.97777778,\n",
       "         0.97777778]),\n",
       " 'split4_train_score': array([ 0.99876238,  1.        ,  0.96534653,  1.        ,  1.        ,\n",
       "         1.        ,  0.99752475,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split5_test_score': array([ 0.95454545,  0.94318182,  0.90909091,  0.94318182,  0.95454545,\n",
       "         0.94318182,  0.95454545,  0.94318182,  0.95454545,  0.94318182,\n",
       "         0.94318182,  0.94318182,  0.95454545,  0.94318182,  0.94318182,\n",
       "         0.94318182]),\n",
       " 'split5_train_score': array([ 0.99876543,  1.        ,  0.97160494,  1.        ,  1.        ,\n",
       "         1.        ,  0.99876543,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split6_test_score': array([ 0.96590909,  0.96590909,  0.94318182,  0.96590909,  0.96590909,\n",
       "         0.96590909,  0.94318182,  0.96590909,  0.96590909,  0.96590909,\n",
       "         0.96590909,  0.96590909,  0.96590909,  0.96590909,  0.96590909,\n",
       "         0.96590909]),\n",
       " 'split6_train_score': array([ 0.99876543,  1.        ,  0.97407407,  1.        ,  1.        ,\n",
       "         1.        ,  0.99876543,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split7_test_score': array([ 0.98863636,  0.95454545,  0.93181818,  0.95454545,  0.98863636,\n",
       "         0.95454545,  0.98863636,  0.95454545,  0.98863636,  0.95454545,\n",
       "         0.96590909,  0.95454545,  0.98863636,  0.95454545,  0.96590909,\n",
       "         0.95454545]),\n",
       " 'split7_train_score': array([ 0.99876543,  1.        ,  0.96790123,  1.        ,  1.        ,\n",
       "         1.        ,  0.99876543,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split8_test_score': array([ 1.        ,  0.98850575,  0.94252874,  0.98850575,  0.98850575,\n",
       "         0.98850575,  1.        ,  0.98850575,  0.98850575,  0.98850575,\n",
       "         1.        ,  0.98850575,  0.98850575,  0.98850575,  1.        ,\n",
       "         0.98850575]),\n",
       " 'split8_train_score': array([ 0.99876695,  1.        ,  0.97163995,  1.        ,  1.        ,\n",
       "         1.        ,  0.99753391,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split9_test_score': array([ 0.98837209,  1.        ,  0.98837209,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split9_train_score': array([ 0.99876847,  1.        ,  0.96551724,  1.        ,  1.        ,\n",
       "         1.        ,  0.99876847,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([ 0.01875637,  0.00237888,  0.00762395,  0.00311857,  0.01286653,\n",
       "         0.00309606,  0.00611688,  0.00925906,  0.00983003,  0.00729571,\n",
       "         0.0034464 ,  0.02108654,  0.01014387,  0.01327131,  0.01637458,\n",
       "         0.01567166]),\n",
       " 'std_score_time': array([ 0.00146413,  0.00176838,  0.00186948,  0.00261147,  0.00328479,\n",
       "         0.00029996,  0.00037442,  0.00020036,  0.0011235 ,  0.00247344,\n",
       "         0.00043629,  0.01059009,  0.00286795,  0.00050303,  0.00915134,\n",
       "         0.00151793]),\n",
       " 'std_test_score': array([ 0.01359986,  0.01590859,  0.02460054,  0.01590859,  0.01359439,\n",
       "         0.01590859,  0.01822677,  0.01590859,  0.01359439,  0.01590859,\n",
       "         0.01836732,  0.01590859,  0.01359439,  0.01590859,  0.01836732,\n",
       "         0.01590859]),\n",
       " 'std_train_score': array([ 0.00037116,  0.        ,  0.00282038,  0.        ,  0.        ,\n",
       "         0.        ,  0.0011422 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=10, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       0.99      1.00      0.99        76\n",
      "          5       0.99      0.97      0.98       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classification example showing the use of GridSearchCV\n",
    "\n",
    "[This example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py) shows how a classifier is optimized by cross-validation on a development set that comprises only half of the available labeled data.\n",
    "\n",
    "The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.\n",
    "\n",
    "**We use optimize the precision and recall measures during grid search. This is useful in case of classification algorithms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.016) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.959 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.026) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.025) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.025) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.975 (+/-0.014) for {'C': 1, 'kernel': 'linear'}\n",
      "0.975 (+/-0.014) for {'C': 10, 'kernel': 'linear'}\n",
      "0.975 (+/-0.014) for {'C': 100, 'kernel': 'linear'}\n",
      "0.975 (+/-0.014) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       1.00      1.00      1.00        76\n",
      "          5       0.99      0.98      0.99       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.019) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.957 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.028) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.972 (+/-0.012) for {'C': 1, 'kernel': 'linear'}\n",
      "0.972 (+/-0.012) for {'C': 10, 'kernel': 'linear'}\n",
      "0.972 (+/-0.012) for {'C': 100, 'kernel': 'linear'}\n",
      "0.972 (+/-0.012) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       1.00      1.00      1.00        76\n",
      "          5       0.99      0.98      0.99       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
