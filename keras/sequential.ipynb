{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential model\n",
    "\n",
    "## [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/#getting-started-with-the-keras-sequential-model)\n",
    "\n",
    "The [Sequential Model](https://keras.io/getting-started/sequential-model-guide/) is a linear stak of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)), # first hidden layer, the first parameter specifies the number of neurons\n",
    "    Activation('relu'),\n",
    "    Dense(10), # the final layer will be the output layer, therefore there are 10 output neurons\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dense](https://keras.io/layers/core/#dense) implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`). Default activation is the identity function. We also have the option to initialize and regularize the kernel and bias. Refer the [initializers](initializers_regularizers.ipynb#Initializers) and [regularizers](initializers_regularizers.ipynb#Regularizers) notebooks respectively.\n",
    " \n",
    "*Note:* If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`.\n",
    "\n",
    "All the possible layers in keras can be [found here](https://keras.io/layers/core/).\n",
    "\n",
    "You can also simply add layers via the `.add()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as first layer in a sequential model:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784)) # input_dim is another way to speify the size of input\n",
    "model.add(Activation('relu'))\n",
    "# now the model will take as input arrays of shape (*, 16)\n",
    "# and output arrays of shape (*, 32)\n",
    "\n",
    "# after the first layer, you don't need to specify\n",
    "# the size of the input anymore:\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Activations](https://keras.io/activations/)\n",
    "\n",
    "Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers. Check out the documentation for the different kinds of activation functions possible.\n",
    "\n",
    "### Input and Output shapes\n",
    "We only need to specify the input shape for the first layer, because the following layers can do automatic shape inference. In `input_shape`, the batch dimension is not included.\n",
    "\n",
    "If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a `batch_size` argument to a layer. The most common situation would be a 2D input with shape (batch_size, input_dim). For a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units).\n",
    "\n",
    "## [Compilation](https://keras.io/getting-started/sequential-model-guide/#compilation)\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "\n",
    "\n",
    "- [optimizer](https://keras.io/optimizers/):  This could be the string identifier of an existing optimizer (such as `rmsprop` or `adagrad`), or an instance of the Optimizer class.\n",
    "```\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "```\n",
    "or\n",
    "```\n",
    "### pass optimizer by name: default parameters will be used\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "```\n",
    "- [loss function](https://keras.io/losses/):  This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `categorical_crossentropy` or `mse`), or it can be an objective function.\n",
    "- [list of metrics](https://keras.io/metrics/):  For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Training](https://keras.io/getting-started/sequential-model-guide/#training)\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the `fit` function.\n",
    "\n",
    "### Example 1: Two-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s - loss: 0.7060 - acc: 0.4900     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6956 - acc: 0.5020     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6919 - acc: 0.5190     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6870 - acc: 0.5190     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6842 - acc: 0.5470     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6837 - acc: 0.5390     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6792 - acc: 0.5540     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6768 - acc: 0.5760     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6723 - acc: 0.5720     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6687 - acc: 0.5870     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d01411dda0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of this model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data = 0.567\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(data)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(pred.round(2)>0.5, labels)\n",
    "print(\"accuracy on training data =\", cmat.diagonal().sum()/cmat.sum())\n",
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving accuracy with more epochs\n",
    "\n",
    "We can improve the accuracy on the training set drastically by iterating for more epochs. But this is not how the number of epochs should be chosen in pratice because this might lead to overfitting.\n",
    "\n",
    "Set `verbose=0` to avoid printing all the interations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data = 0.74\n"
     ]
    }
   ],
   "source": [
    "model.fit(data, labels, epochs=50, batch_size=32, verbose=0)\n",
    "pred = model.predict(data)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(pred.round(2)>0.5, labels)\n",
    "print(\"accuracy on training data =\", cmat.diagonal().sum()/cmat.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the model\n",
    "```\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='bin_classification.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  1  3  1  4  1  2  2  2  3]\n",
      " [ 9 83  6 10  4 10  8  7  6  5]\n",
      " [ 8  3 62  4  1  3  4  6  1  5]\n",
      " [11  3  7 73  9  4  6  7  3  3]\n",
      " [ 7  1  2 13 71  5  5  5  5  4]\n",
      " [ 6  3  4  2  6 74  6  3  6  3]\n",
      " [ 2  2  0  2  0  0 58  7  0  3]\n",
      " [ 1  1  1  0  5  0  1 58  3  7]\n",
      " [ 6  3  8  5  2  5  8 11 71 10]\n",
      " [ 2  1  1  0  4  0  1  5  2 45]]\n",
      "accuracy on training data = 0.633\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=150, batch_size=32, verbose = 0)\n",
    "\n",
    "# Accuracy on the training data\n",
    "pred = model.predict(data)\n",
    "\n",
    "# taking the maximum output as the predicted class label and then building the confusion matrix\n",
    "cmat = confusion_matrix(pred.argmax(axis = 1), labels)\n",
    "print(cmat)\n",
    "print(\"accuracy on training data =\", cmat.diagonal().sum()/cmat.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "\n",
    "`Dropout` consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.4145 - acc: 0.0960     \n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3590 - acc: 0.1120     \n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3489 - acc: 0.0900     \n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3335 - acc: 0.1060     \n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3317 - acc: 0.1050     \n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3056 - acc: 0.1020     \n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3135 - acc: 0.1200     \n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3153 - acc: 0.1270     \n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3084 - acc: 0.1140     \n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3062 - acc: 0.1130     \n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3079 - acc: 0.1020     \n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3087 - acc: 0.1050     \n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3141 - acc: 0.1140     \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3013 - acc: 0.1020     \n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3052 - acc: 0.1220     \n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3042 - acc: 0.0900     \n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3047 - acc: 0.1120     \n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3025 - acc: 0.1140     \n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2977 - acc: 0.1030     \n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3019 - acc: 0.0990     \n",
      "100/100 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3028013706207275, 0.10000000149011612]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
